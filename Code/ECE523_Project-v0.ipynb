{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "f8fed4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, TFBertModel, BertConfig\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from ast import literal_eval\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "839c7684",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f1a941a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    def __init__(self):\n",
    "        return\n",
    "\n",
    "    def organizeData(self, featuresFile, patientNotesFile, trainFile):\n",
    "        self.fDF = pd.read_csv(featuresFile)\n",
    "        self.pnDF = pd.read_csv(patientNotesFile)\n",
    "        self.tDF = pd.read_csv(trainFile)\n",
    "\n",
    "        self.tDF['annotation_list'] = self.tDF['annotation'].apply(literal_eval)\n",
    "        self.tDF['location_list'] = self.tDF['location'].apply(literal_eval)\n",
    "\n",
    "        self.removeORs()\n",
    "        self.removeSpecialChars()\n",
    "        \n",
    "        self.merged = self.tDF.merge(self.pnDF, how=\"left\")\n",
    "        self.merged = self.merged.merge(self.fDF, how=\"left\")\n",
    "        self.merged['annotation_length'] = self.tDF['annotation'].apply(len)\n",
    "\n",
    "    def removeORs(self):\n",
    "        self.fDF['feature_text'] = self.fDF['feature_text'].apply(lambda x: x.lower())\n",
    "        for i in range(len(self.fDF['feature_text'])):\n",
    "            self.fDF.at[i, 'feature_text'] = self.fDF['feature_text'][i].replace(\"-OR-\", \";-\").replace(\"-\", \" \")\n",
    "\n",
    "    def removeSpecialChars(self):\n",
    "        self.pnDF['pn_history'] = self.pnDF['pn_history'].apply(lambda x: x.lower())\n",
    "        for i in range(len(self.pnDF['pn_history'])):\n",
    "            self.pnDF.at[i, 'pn_history'] = self.pnDF['pn_history'][i].replace(\"(\", \" \")\n",
    "            self.pnDF.at[i, 'pn_history'] = self.pnDF['pn_history'][i].replace(\")\", \" \")\n",
    "            self.pnDF.at[i, 'pn_history'] = self.pnDF['pn_history'][i].replace(\":\", \" \")\n",
    "            self.pnDF.at[i, 'pn_history'] = self.pnDF['pn_history'][i].replace(\";\", \" \")\n",
    "            self.pnDF.at[i, 'pn_history'] = self.pnDF['pn_history'][i].replace(\"-\", \" \")\n",
    "            self.pnDF.at[i, 'pn_history'] = self.pnDF['pn_history'][i].replace(\"/\", \" \")\n",
    "            self.pnDF.at[i, 'pn_history'] = self.pnDF['pn_history'][i].replace(\"\\\\\", \" \")\n",
    "            self.pnDF.at[i, 'pn_history'] = self.pnDF['pn_history'][i].replace(\"\\r\\n\", \"  \")\n",
    "            self.pnDF.at[i, 'pn_history'] = self.pnDF['pn_history'][i].replace(\"\\'\", \" \")\n",
    "            self.pnDF.at[i, 'pn_history'] = self.pnDF['pn_history'][i].replace(\"\\\"\", \" \")\n",
    "            self.pnDF.at[i, 'pn_history'] = self.pnDF['pn_history'][i].replace(\",\", \" \")\n",
    "            self.pnDF.at[i, 'pn_history'] = self.pnDF['pn_history'][i].replace(\".\", \" \")\n",
    "            self.pnDF.at[i, 'pn_history'] = re.sub(r'\\bzero\\b', ' 0  ', self.pnDF['pn_history'][i])\n",
    "            self.pnDF.at[i, 'pn_history'] = re.sub(r'\\bone\\b', ' 1 ', self.pnDF['pn_history'][i])\n",
    "            self.pnDF.at[i, 'pn_history'] = re.sub(r'\\btwo\\b', ' 2 ', self.pnDF['pn_history'][i])\n",
    "            self.pnDF.at[i, 'pn_history'] = re.sub(r'\\bthree\\b', '  3  ', self.pnDF['pn_history'][i])\n",
    "            self.pnDF.at[i, 'pn_history'] = re.sub(r'\\bfour\\b', ' 4  ', self.pnDF['pn_history'][i])\n",
    "            self.pnDF.at[i, 'pn_history'] = re.sub(r'\\bfive\\b', ' 5  ', self.pnDF['pn_history'][i])\n",
    "            self.pnDF.at[i, 'pn_history'] = re.sub(r'\\bsix\\b', ' 6 ', self.pnDF['pn_history'][i])\n",
    "            self.pnDF.at[i, 'pn_history'] = re.sub(r'\\bseven\\b', '  7  ', self.pnDF['pn_history'][i])\n",
    "            self.pnDF.at[i, 'pn_history'] = re.sub(r'\\beight\\b', '  8  ', self.pnDF['pn_history'][i])\n",
    "            self.pnDF.at[i, 'pn_history'] = re.sub(r'\\bnine\\b', ' 9  ', self.pnDF['pn_history'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "06ff06f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "family history of mi or family history of myocardial infarction\n",
      "17 year old male  has come to the student health clinic complaining of heart pounding  mr  cleveland s mother has given verbal consent for a history  physical examination  and treatment   began 2 3 months ago sudden intermittent for 2 days lasting 3 4 min  worsening non allev aggrav   associated with dispnea on exersion and rest stressed out about school   reports fe feels like his heart is jumping out of his chest   ros denies chest pain dyaphoresis wt loss chills fever nausea vomiting pedal edeam   pmh non meds  aderol  from a friend  nkda   fh father had mi recently mother has thyroid dz   sh non smoker mariguana 5 6 months ago 3 beers on the weekend  basketball at school   sh no std\n"
     ]
    }
   ],
   "source": [
    "featuresFile = \"..\\\\CSVs\\\\features.csv\"\n",
    "patientNotesFile = \"..\\\\CSVs\\\\patient_notes.csv\"\n",
    "trainFile = \"..\\\\CSVs\\\\train.csv\"\n",
    "\n",
    "prep = Preprocessing()\n",
    "prep.organizeData(featuresFile, patientNotesFile, trainFile)\n",
    "\n",
    "print(prep.fDF['feature_text'][0])\n",
    "print(prep.pnDF['pn_history'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ea2eb6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens 162\n",
      "['family', 'history', 'of', 'mi', 'or', 'family', 'history', 'of', 'my', '##oca', '##rdial', 'in', '##far', '##ction', '17', 'year', 'old', 'male', 'has', 'come', 'to', 'the', 'student', 'health', 'clinic', 'complaining', 'of', 'heart', 'pounding', 'mr', 'cleveland', 's', 'mother', 'has', 'given', 'verbal', 'consent', 'for', 'a', 'history', 'physical', 'examination', 'and', 'treatment', 'began', '2', '3', 'months', 'ago', 'sudden', 'intermittent', 'for', '2', 'days', 'lasting', '3', '4', 'min', 'worse', '##ning', 'non', 'all', '##ev', 'ag', '##gra', '##v', 'associated', 'with', 'di', '##sp', '##nea', 'on', 'ex', '##ers', '##ion', 'and', 'rest', 'stressed', 'out', 'about', 'school', 'reports', 'fe', 'feels', 'like', 'his', 'heart', 'is', 'jumping', 'out', 'of', 'his', 'chest', 'ro', '##s', 'denies', 'chest', 'pain', 'd', '##ya', '##ph', '##ores', '##is', 'w', '##t', 'loss', 'chill', '##s', 'fever', 'nausea', 'vomiting', 'pedal', 'ed', '##ea', '##m', 'pm', '##h', 'non', 'med', '##s', 'ad', '##ero', '##l', 'from', 'a', 'friend', 'nk', '##da', 'f', '##h', 'father', 'had', 'mi', 'recently', 'mother', 'has', 'thyroid', 'd', '##z', 'sh', 'non', 'smoke', '##r', 'mari', '##gua', '##na', '5', '6', 'months', 'ago', '3', 'beers', 'on', 'the', 'weekend', 'basketball', 'at', 'school', 'sh', 'no', 'st', '##d']\n",
      "{'input_ids': [101, 2459, 2095, 2214, 3287, 2038, 2272, 2000, 1996, 3076, 2740, 9349, 17949, 1997, 2540, 9836, 2720, 6044, 1055, 2388, 2038, 2445, 12064, 9619, 2005, 1037, 2381, 3558, 7749, 1998, 3949, 2211, 1016, 1017, 2706, 3283, 5573, 23852, 2005, 1016, 2420, 9879, 1017, 1018, 8117, 4788, 5582, 2512, 2035, 6777, 12943, 17643, 2615, 3378, 2007, 4487, 13102, 22084, 2006, 4654, 2545, 3258, 1998, 2717, 13233, 2041, 2055, 2082, 4311, 10768, 5683, 2066, 2010, 2540, 2003, 8660, 2041, 1997, 2010, 3108, 20996, 2015, 23439, 3108, 3255, 1040, 3148, 8458, 16610, 2483, 1059, 2102, 3279, 10720, 2015, 9016, 19029, 24780, 15749, 3968, 5243, 2213, 7610, 2232, 2512, 19960, 2015, 4748, 10624, 2140, 2013, 1037, 2767, 25930, 2850, 1042, 2232, 2269, 2018, 2771, 3728, 2388, 2038, 29610, 1040, 2480, 14021, 2512, 5610, 2099, 16266, 19696, 2532, 1019, 1020, 2706, 3283, 1017, 18007, 2006, 1996, 5353, 3455, 2012, 2082, 14021, 2053, 2358, 2094, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "tokens = tokenizer.tokenize(prep.fDF['feature_text'][0], prep.pnDF['pn_history'][0])\n",
    "\n",
    "print(\"Total Tokens\", len(tokens))\n",
    "print(tokens)\n",
    "print(tokenizer(prep.pnDF['pn_history'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c3395b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mom with  thyroid disease\n",
      "[(0, 0), (0, 6), (7, 14), (15, 17), (18, 20), (21, 23), (24, 30), (31, 38), (39, 41), (42, 44), (44, 47), (47, 52), (53, 55), (55, 58), (58, 63), (0, 0), (0, 2), (2, 3), (5, 7), (7, 9), (10, 11), (12, 20), (21, 25), (26, 29), (29, 32), (32, 38), (40, 47), (48, 55), (56, 57), (58, 59), (60, 66), (67, 69), (70, 82), (83, 91), (92, 94), (96, 101), (102, 109), (110, 118), (119, 122), (123, 125), (126, 128), (129, 134), (137, 138), (139, 143), (144, 147), (148, 154), (155, 156), (157, 163), (164, 168), (169, 172), (173, 175), (176, 183), (185, 188), (189, 193), (194, 198), (199, 202), (203, 208), (209, 217), (218, 221), (222, 226), (227, 229), (230, 232), (233, 235), (236, 240), (241, 246), (247, 249), (250, 254), (255, 258), (260, 263), (264, 267), (268, 272), (273, 276), (276, 281), (281, 285), (288, 290), (291, 295), (296, 303), (304, 307), (307, 310), (310, 312), (313, 316), (316, 320), (321, 324), (324, 328), (328, 329), (331, 340), (341, 343), (344, 349), (351, 352), (353, 354), (355, 360), (361, 364), (365, 369), (372, 378), (379, 385), (386, 392), (393, 397), (399, 403), (404, 407), (407, 409), (409, 412), (412, 413), (414, 419), (420, 426), (427, 430), (431, 438), (439, 441), (442, 446), (448, 454), (455, 460), (460, 464), (465, 467), (468, 474), (476, 479), (479, 481), (481, 485), (485, 487), (489, 494), (494, 495), (497, 502), (502, 503), (505, 513), (515, 522), (524, 531), (532, 534), (535, 540), (542, 549), (550, 552), (553, 559), (560, 567), (569, 578), (579, 582), (582, 583), (585, 592), (593, 595), (596, 599), (599, 601), (602, 604), (605, 607), (607, 610), (610, 612), (613, 619), (623, 625), (625, 626), (626, 627), (629, 633), (635, 636), (636, 637), (639, 643), (644, 651), (652, 655), (655, 657), (657, 660), (660, 661), (663, 664), (664, 665), (665, 666), (668, 671), (672, 676), (678, 685), (686, 693), (696, 699), (700, 704), (705, 711), (712, 717), (718, 720), (720, 722), (722, 724), (726, 729), (731, 735), (737, 739), (739, 742), (742, 750), (752, 754), (755, 757), (758, 762), (764, 766), (766, 767), (769, 777), (778, 780), (781, 788), (790, 793), (793, 796), (796, 798), (799, 800), (801, 802), (803, 809), (810, 811), (812, 818), (821, 825), (827, 829), (830, 838), (841, 847), (848, 851), (851, 853), (853, 855), (857, 860), (860, 863), (863, 865), (866, 872), (873, 882), (884, 892), (893, 899), (900, 904), (905, 915), (916, 917), (918, 919), (920, 924), (926, 930), (931, 938), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)]\n",
      "Seq ID zero, so level is -1 also\n",
      "Word mom, label: 1\n",
      "Word with, label: 1\n",
      "Word thyroid, label: 1\n",
      "Word disease, label: 1\n"
     ]
    }
   ],
   "source": [
    "tokenized_list = tokenizer(\n",
    "        prep.merged.iloc[0].feature_text,\n",
    "        prep.merged.iloc[0].pn_history,\n",
    "        truncation=True,\n",
    "        max_length=1000,\n",
    "        padding='max_length',\n",
    "        return_offsets_mapping=True\n",
    ")\n",
    "print(prep.merged.iloc[0].pn_history[668:693])\n",
    "\n",
    "zipped = zip(tokenized_list.sequence_ids(), tokenized_list[\"offset_mapping\"])\n",
    "\n",
    "idx, (seq_id, offsets) = next(enumerate(zipped))\n",
    "if not seq_id or seq_id == 0:\n",
    "    print(\"Seq ID zero, so level is -1 also\")\n",
    "\n",
    "seq_id = 1 #assume\n",
    "loc_list = [668, 693]\n",
    "\n",
    "for idx, (seq_id, offsets)  in enumerate(zip(tokenized_list.sequence_ids(), tokenized_list[\"offset_mapping\"])):\n",
    "    token_start, token_end = offsets\n",
    "    for feature_start, feature_end in [loc_list]:\n",
    "        if token_start >= feature_start and token_end <= feature_end:\n",
    "            print(f\"Word {prep.merged.iloc[0].pn_history[token_start:token_end]}, label: 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "614fd666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    ## BERT encoder\n",
    "    configuration = BertConfig().from_pretrained('bert-base-uncased',output_attentions=False,output_hidden_states=False,return_dict =True)\n",
    "    encoder = TFBertModel(configuration)#.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    ## QA Model\n",
    "    input_ids = layers.Input(shape=(max_len,), dtype=tf.int32, name = 'input_ids')\n",
    "    token_type_ids = layers.Input(shape=(max_len,), dtype=tf.int32, name = 'token_type_ids')\n",
    "    attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32, name = 'attention_mask')\n",
    "    embedding = encoder(\n",
    "        input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask\n",
    "    )[0]\n",
    "\n",
    "    start_logits = layers.Dense(1, name=\"start_logit\", use_bias=False)(embedding)\n",
    "    start_logits = layers.Flatten()(start_logits)\n",
    "\n",
    "    end_logits = layers.Dense(1, name=\"end_logit\", use_bias=False)(embedding)\n",
    "    end_logits = layers.Flatten()(end_logits)\n",
    "\n",
    "    start_probs = layers.Activation(keras.activations.softmax)(start_logits)\n",
    "    end_probs = layers.Activation(keras.activations.softmax)(end_logits)\n",
    "\n",
    "    model = keras.Model(\n",
    "        inputs=[input_ids, token_type_ids, attention_mask],\n",
    "        outputs=[start_probs, end_probs],\n",
    "    )\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=5e-5)\n",
    "    model.compile(optimizer=optimizer, loss=[loss, loss])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "57d6ab47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    }
   ],
   "source": [
    "max_len = 384\n",
    "#configuration = BertConfig()\n",
    "with tf.device('/cpu:0'):\n",
    "    model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "0251a1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 384)]        0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 384)]        0           []                               \n",
      "                                                                                                  \n",
      " token_type_ids (InputLayer)    [(None, 384)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model_11 (TFBertModel)  TFBaseModelOutputWi  109482240  ['input_ids[0][0]',              \n",
      "                                thPooling(last_hidd               'attention_mask[0][0]',         \n",
      "                                en_state=(None, 384               'token_type_ids[0][0]']         \n",
      "                                , 768),                                                           \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None)                                                \n",
      "                                                                                                  \n",
      " start_logit (Dense)            (None, 384, 1)       768         ['tf_bert_model_11[0][0]']       \n",
      "                                                                                                  \n",
      " end_logit (Dense)              (None, 384, 1)       768         ['tf_bert_model_11[0][0]']       \n",
      "                                                                                                  \n",
      " flatten_14 (Flatten)           (None, 384)          0           ['start_logit[0][0]']            \n",
      "                                                                                                  \n",
      " flatten_15 (Flatten)           (None, 384)          0           ['end_logit[0][0]']              \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 384)          0           ['flatten_14[0][0]']             \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 384)          0           ['flatten_15[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,483,776\n",
      "Trainable params: 109,483,776\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "780f4d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                                                         00016_000\n",
      "case_num                                                           0\n",
      "pn_num                                                            16\n",
      "feature_num                                                        0\n",
      "annotation                          ['dad with recent heart attcak']\n",
      "annotation_list                       [dad with recent heart attcak]\n",
      "location_list                                              [696 724]\n",
      "pn_history         hpi  17yo m presents with palpitations  patien...\n",
      "feature_text       family history of mi or family history of myoc...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(prep.merged.drop(columns=['location']).iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "48a1acb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "['696 724']\n"
     ]
    }
   ],
   "source": [
    "print(type(prep.merged['location_list'].iloc[0][0]))\n",
    "print(prep.merged['location_list'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "79192a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label(text, annotation_length, location_list):\n",
    "    #tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "    encoded = tokenizer(text,\n",
    "                        add_special_tokens=False,\n",
    "                        max_length=max_len,\n",
    "                        padding=\"max_length\",\n",
    "                        return_offsets_mapping=True)\n",
    "    offset_mapping = encoded['offset_mapping']\n",
    "    ignore_idxes = np.where(np.array(encoded.sequence_ids()) != 0)[0]\n",
    "    label = np.zeros(len(offset_mapping))\n",
    "    label[ignore_idxes] = -1\n",
    "    if annotation_length != 0:\n",
    "        for location in location_list:\n",
    "            for loc in [s.split() for s in location.split(';')]:\n",
    "                start_idx = -1\n",
    "                end_idx = -1\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                for idx in range(len(offset_mapping)):\n",
    "                    if (start_idx == -1) & (start < offset_mapping[idx][0]):\n",
    "                        start_idx = idx - 1\n",
    "                    if (end_idx == -1) & (end <= offset_mapping[idx][1]):\n",
    "                        end_idx = idx + 1\n",
    "                if start_idx == -1:\n",
    "                    start_idx = end_idx\n",
    "                if (start_idx != -1) & (end_idx != -1):\n",
    "                    label[start_idx:end_idx] = 1\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "d037aa2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  1  1  1  1  1  1  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "(384,)\n",
      "annotation location:  ['203 217']\n",
      "chest pressure\n",
      "\n",
      "label = 1: (array([145, 146, 147, 148, 149, 150, 151], dtype=int64),)\n",
      "tokenized_text[181:187]: ['chest', 'pressure']\n"
     ]
    }
   ],
   "source": [
    "label = create_label(prep.merged['pn_history'][0], prep.merged['annotation_length'][0], prep.merged['location_list'][0])\n",
    "label = np.asarray(label, dtype=int).reshape(1, max_len)[0]\n",
    "print(label)\n",
    "print(label.shape)\n",
    "\n",
    "print(\"annotation location: \", prep.merged['location_list'][2])\n",
    "print(prep.merged['pn_history'][2][203:217])\n",
    "\n",
    "print(f'\\nlabel = 1: {np.where(label == 1)}')\n",
    "tokenized_text = tokenizer.tokenize(prep.merged['pn_history'][2], add_special_tokens=False)\n",
    "print(f'tokenized_text[181:187]: {tokenized_text[40:42]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "fec0eb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[  101  6522  2072  2459  7677  1049  7534  2007 14412 23270 10708  5776\n",
      "  4311  1017  1018  2706  1997 23852  4178  1997  2540  6012  9836  2041\n",
      "  1997  2026  3108  1016  2420  3283  2076  1037  4715  2208  2018  2019\n",
      "  2792  2021  2023  2051  2018  3108  3778  1998  2371  2004  2065  2002\n",
      "  2020  2183  2000  3413  2041  2106  2025  4558  9530 18436  2791  1997\n",
      "  3602  5776  2203  5668  2229  8273  7741  5587 21673  2140  3952  2000\n",
      "  2817  1015  1017  2335  2566  2733  2077  3522  4715  2208  2165  5587\n",
      "  2121  7941  2140  2305  2077  1998  2851  1997  2208 23439  2460  2791\n",
      "  1997  3052 22939  8458 16610  2483  9016  2015 10720  2015 14978 16342\n",
      "  3431  1999  3637  3431  1999  4432  4994 21419 29025  2078  3431  1999\n",
      "  6812  2884  2030 24471  3981  2854 14243  7610  2232  2595  3904  1054\n",
      "  2595  3594  2814  5587  2121  7941  2140  1042  2232  2595  3566  2007\n",
      " 29610  4295  3611  2007  3522  2540  2012 13535  4817  2035  3904 10047\n",
      " 23041 22318  2039  2000  3058 14021  2595 26612  1999  2267  2203  5668\n",
      "  2229  1017  1018  8974  1017  6385  2733  2006 13499 23439 21628  6305\n",
      "  3597  2203  5668  2229  2667 16204 12581  3161  2007  6513  1060  1015\n",
      "  2095  3594 29094   102  2155  2381  1997  2771  2030  2155  2381  1997\n",
      "  2026 24755 25070  1999 14971  7542   102     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "   101  2459 10930  1049  1059  2053 15050  2030 12098 25032 22123 26837\n",
      "  2050  7610  2232  7534 17949  1997  1017  2706  1997  4958 19565 14808\n",
      "  2540  9836 13866  4311  8742  1997  2540  3868  1998  9836  2041  1997\n",
      "  3108  2055  1019  1020  2335  2561  2058  1996  2627  1017  2706  2023\n",
      "  2197  2792  1996  5776  5281  2422  3753  2791  1998  2018  2070  2460\n",
      "  2791  1997  3052 13866  2003  1037  2267  3076  1998  2038  3728  2211\n",
      "  2000  3745  2010 18328  2015 16250  4748 21673  2140 22480  1016 21628\n",
      "  2015  2733  2058  2023  2051 13866  2515  2025  3189  2151 15316  3896\n",
      "  1997  1996  4748 21673  2140  1998  4311  2009  2038  3271  2032  2817\n",
      "  2053  3431  1999  3635  4487 29212  3431  1999 18923  9016  2015  2030\n",
      " 10720  2015  7610  2232  3904  8827  2232  3904  1042  2232  2595  2269\n",
      "  2771  3566 29610  3291 19960  2015  3904  5138  4748 21673  2140  2025\n",
      " 16250  2591  2267  3076  1999  4695  1016  1017 14813  8974  2733  2515\n",
      "  2025  5610  2699 16204  2320 12581  3161  1059  1015  2931  4256  3594\n",
      " 29094 20996  2015  4997  5138  2682   102  2155  2381  1997 29610  8761\n",
      "   102     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "input_data = tokenizer(\n",
    "        prep.merged.iloc[0].pn_history,\n",
    "        prep.merged.iloc[0].feature_text,\n",
    "        #truncation=True,\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        return_offsets_mapping=False\n",
    ")\n",
    "#print(input_data)\n",
    "test_input_data = {}\n",
    "test_input_data['input_ids'] = np.asarray(input_data['input_ids'], dtype=int).reshape(1,max_len)\n",
    "test_input_data['token_type_ids'] = np.asarray(input_data['token_type_ids'], dtype=int).reshape(1,max_len)\n",
    "test_input_data['attention_mask'] = np.asarray(input_data['attention_mask'], dtype=int).reshape(1,max_len)\n",
    "input_data = tokenizer(\n",
    "        prep.merged.iloc[40].pn_history,\n",
    "        prep.merged.iloc[40].feature_text,\n",
    "        #truncation=True,\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        return_offsets_mapping=False\n",
    ")\n",
    "test_input_data['input_ids']=np.append(test_input_data['input_ids'],np.asarray(input_data['input_ids'], dtype=int).reshape(1,max_len))\n",
    "test_input_data['token_type_ids'] = np.asarray(input_data['token_type_ids'], dtype=int).reshape(1,max_len)\n",
    "test_input_data['attention_mask'] = np.asarray(input_data['attention_mask'], dtype=int).reshape(1,max_len)\n",
    "print(type(test_input_data['input_ids']))\n",
    "print(test_input_data['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "6dce9fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(384,)\n",
      "(384,)\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 384) for input KerasTensor(type_spec=TensorSpec(shape=(None, 384), dtype=tf.int32, name='input_ids'), name='input_ids', description=\"created by layer 'input_ids'\"), but it was called on an input with incompatible shape (1,).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 384) for input KerasTensor(type_spec=TensorSpec(shape=(None, 384), dtype=tf.int32, name='token_type_ids'), name='token_type_ids', description=\"created by layer 'token_type_ids'\"), but it was called on an input with incompatible shape (1,).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 384) for input KerasTensor(type_spec=TensorSpec(shape=(None, 384), dtype=tf.int32, name='attention_mask'), name='attention_mask', description=\"created by layer 'attention_mask'\"), but it was called on an input with incompatible shape (1,).\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "ename": "StagingError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\Serhan\\anaconda3\\envs\\tf-cpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Serhan\\anaconda3\\envs\\tf-cpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Serhan\\anaconda3\\envs\\tf-cpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Serhan\\anaconda3\\envs\\tf-cpu\\lib\\site-packages\\keras\\engine\\training.py\", line 859, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\Serhan\\anaconda3\\envs\\tf-cpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    StagingError: Exception encountered when calling layer \"tf_bert_model_16\" (type TFBertModel).\n    \n    in user code:\n    \n        File \"C:\\Users\\Serhan\\anaconda3\\envs\\tf-cpu\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 887, in call  *\n            outputs = self.bert(\n        File \"C:\\Users\\Serhan\\anaconda3\\envs\\tf-cpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n    \n        StagingError: Exception encountered when calling layer \"bert\" (type TFBertMainLayer).\n        \n        in user code:\n        \n            File \"C:\\Users\\Serhan\\anaconda3\\envs\\tf-cpu\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 658, in call  *\n                extended_attention_mask = tf.reshape(inputs[\"attention_mask\"], (input_shape[0], 1, 1, input_shape[1]))\n        \n            IndexError: list index out of range\n        \n        \n        Call arguments received:\n          • input_ids=tf.Tensor(shape=(1,), dtype=int32)\n          • attention_mask=tf.Tensor(shape=(1,), dtype=int32)\n          • token_type_ids=tf.Tensor(shape=(1,), dtype=int32)\n          • position_ids=None\n          • head_mask=None\n          • inputs_embeds=None\n          • output_attentions=False\n          • output_hidden_states=False\n          • return_dict=True\n          • training=True\n          • kwargs=<class 'inspect._empty'>\n    \n    \n    Call arguments received:\n      • input_ids=tf.Tensor(shape=(1,), dtype=int32)\n      • attention_mask=tf.Tensor(shape=(1,), dtype=int32)\n      • token_type_ids=tf.Tensor(shape=(1,), dtype=int32)\n      • position_ids=None\n      • head_mask=None\n      • inputs_embeds=None\n      • output_attentions=None\n      • output_hidden_states=None\n      • return_dict=None\n      • training=True\n      • kwargs=<class 'inspect._empty'>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStagingError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9744\\2770055413.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# For demonstration, 3 epochs are recommended\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-cpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-cpu\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1146\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1147\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1148\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mStagingError\u001b[0m: in user code:\n\n    File \"C:\\Users\\Serhan\\anaconda3\\envs\\tf-cpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Serhan\\anaconda3\\envs\\tf-cpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Serhan\\anaconda3\\envs\\tf-cpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Serhan\\anaconda3\\envs\\tf-cpu\\lib\\site-packages\\keras\\engine\\training.py\", line 859, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\Serhan\\anaconda3\\envs\\tf-cpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    StagingError: Exception encountered when calling layer \"tf_bert_model_16\" (type TFBertModel).\n    \n    in user code:\n    \n        File \"C:\\Users\\Serhan\\anaconda3\\envs\\tf-cpu\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 887, in call  *\n            outputs = self.bert(\n        File \"C:\\Users\\Serhan\\anaconda3\\envs\\tf-cpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n    \n        StagingError: Exception encountered when calling layer \"bert\" (type TFBertMainLayer).\n        \n        in user code:\n        \n            File \"C:\\Users\\Serhan\\anaconda3\\envs\\tf-cpu\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 658, in call  *\n                extended_attention_mask = tf.reshape(inputs[\"attention_mask\"], (input_shape[0], 1, 1, input_shape[1]))\n        \n            IndexError: list index out of range\n        \n        \n        Call arguments received:\n          • input_ids=tf.Tensor(shape=(1,), dtype=int32)\n          • attention_mask=tf.Tensor(shape=(1,), dtype=int32)\n          • token_type_ids=tf.Tensor(shape=(1,), dtype=int32)\n          • position_ids=None\n          • head_mask=None\n          • inputs_embeds=None\n          • output_attentions=False\n          • output_hidden_states=False\n          • return_dict=True\n          • training=True\n          • kwargs=<class 'inspect._empty'>\n    \n    \n    Call arguments received:\n      • input_ids=tf.Tensor(shape=(1,), dtype=int32)\n      • attention_mask=tf.Tensor(shape=(1,), dtype=int32)\n      • token_type_ids=tf.Tensor(shape=(1,), dtype=int32)\n      • position_ids=None\n      • head_mask=None\n      • inputs_embeds=None\n      • output_attentions=None\n      • output_hidden_states=None\n      • return_dict=None\n      • training=True\n      • kwargs=<class 'inspect._empty'>\n"
     ]
    }
   ],
   "source": [
    "print(test_input_data['attention_mask'].shape)\n",
    "print(label.shape)\n",
    "#configuration = BertConfig().from_pretrained('bert-base-uncased')\n",
    "#model = AutoModel.from_pretrained('bert-base-uncased', config = configuration)\n",
    "model.fit(\n",
    "    test_input_data,\n",
    "    label,\n",
    "    epochs=1,  # For demonstration, 3 epochs are recommended\n",
    "    verbose=2,\n",
    "    batch_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "a2e9c364",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME,normalization=True)\n",
    "config = AutoConfig.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e1d367",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
